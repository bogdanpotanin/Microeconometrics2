# --------
# Коссова Е.В., Потанин Б.С.
# Микроэконометрика качественных данных
# Тема 2. Альтернативные спецификации распределений
#         в моделях бинарного выбора
# --------

# Отключим scientific notation
options(scipen = 999)

#---------------------------------------------------
# Симуляция данных
#---------------------------------------------------

# Воспроизведем процесс генерации данных, предполагаемый 
# классическими моделями бинарного выбора с линейным 
# индексом.

# Для удобства представим, что мы симулируем процесс,
# определяющий дефолт по кредиту.

# Симулируем данные
set.seed(123)                                        # для воспроизводимости
n <- 10000                                           # число индивидов в выборке
h <- data.frame(income = exp(rnorm(n, 10, 0.7)))     # доход
h$age = round(runif(n, 20, 100))                     # возраст
educ = t(rmultinom(n, 1, c(0.5, 0.3, 0.2)))          # уровень образования
h$educ_1 = as.numeric(educ[, 1] == 1)                # среднее образование
h$educ_2 = as.numeric(educ[, 2] == 1)                # среднее специальное образование
h$educ_3 <- as.numeric(educ[, 3] == 1)               # высшее образование
h$credit <- runif(n, 10000, 1000000)                 # объем кредита
h$stable <- rbinom(n, 1, 0.6)                        # стабильная работа

# Симулируем случайную ошибку 
# несколькими различными способами
  # Случай №1
eps <- rnorm(n)                                      # случайная ошибка из
                                                     # стандартного нормального
                                                     # распределения
plot(density(eps))
  # Случай №2
ber <- rbinom(n, size = 1, prob = 0.7)               # случайная ошибка
xi1 <- rt(n, 5)                                      # из смеси распределений
xi2 <- rt(n, 5)                                      # Стьюдента
eps2 <- ber * (xi1 - 3) + 
        (1 - ber) * (xi2 + 3)
plot(density(eps2))
  # Случай №3
tau <- c(0.01, 0.05)
eps3_var <- exp(tau[1] * h$age +                     # дисперсия случайной ошибки
                tau[2] * h$stable) ^ 2               # зависит от некоторых регрессоров
eps3 <- rnorm(n, 0, sqrt(eps3_var))                  # случайная ошибка с гетероскедастичной
                                                     # случайно ошибки
plot(density(eps3))
     
# Создадим линейный индекс                                  
beta <- c(8, -0.7, -0.02,                            # оцениваемые регрессионные 
          0.0001, -0.1, -0.3,                        # коэффициенты
          -0.5, 0.001, -0.1)   

default_li <- beta[1] + 
              beta[2] * log(h$income) +              # линейный индекс,
              beta[3] * h$age +                      # отражающий вклад наблюдаемых
              beta[4] * h$age ^ 2 +                  # факторов в вероятность дефолта
              beta[5] * h$educ_1 +
              beta[6] * h$educ_2 +
              beta[7] * h$educ_3 +
              beta[8] * sqrt(h$credit) +
              beta[9] * h$stable * log(h$income)

default_star <- default_li + eps                     # латентная переменная,
                                                     # отражающая склонность
                                                     # к дефолту
default_star2 <- default_li + eps2                   # латентная переменная с
                                                     # с не нормальной случайно ошибкой
default_star3 <- default_li + eps3                   # латентная переменная с
                                                     # гетероскедастичной случайной ошибкой

# Создадим наблюдаемую зависимую переменную,
# отражающую факт дефолта
h$default <- as.numeric(default_star >= 0)           # наблюдаемое значение переменной
mean(h$default)                                      # доля дефолтов
h$default2 <- as.numeric(default_star2 >= 0)
mean(h$default2)
h$default3 <- as.numeric(default_star3 >= 0)
mean(h$default3)

# Итоговые данные
head(h, 10)

# --------------------------------------------
# Описание переменных:
# income    - доход
# age       - возраст
# educ_1    - среднее образование
# educ_2    - среднее специальное образование
# educ_3    - высшее образование
# credit    - объем кредита
# default   - факт дефолта
# default2  - факт дефолта при не нормальных
#             случайных ошибках
# default3  - факт дефолта при гетероскедастичных
#             случайных ошибках
# stable    - стабильная работа
# --------------------------------------------

#---------------------------------------------------
# Часть 1. Учет гетероскедастичности в пробит модели
#---------------------------------------------------

# -----
# Учимся:
# 1. Оценивать пробит модель с гетероскедастичной
#    случайно ошибкой
# 2. Считать для данной модели вероятности 
#    и предельные эффекты
# 3. Проверять гипотезу о гомоскедастичности
# -----

library("glmx")                                                  # пакет, позволяющий оценивать пробит
                                                                 # модель с гетероскдестичной 
                                                                 # случайной ошибкой

library("lmtest")                                                # дополнительные тесты

library("numDeriv")                                              # численное дифференцирование

library("margins")                                               # расчет предельных эффектов

library("hpa")                                                   # распределение Галланта и Нички

# Оценим пробит модель
model_probit <- glm(formula = default3 ~ log(income) +           # указываем формулу без константы, поскольку
                                         age + educ_3,           # она учитывается автоматически
                    data = h,                                    # датафрейм, из которого берутся 
                                                                 # зависимая и независимые переменные
                    family = binomial(link = "probit"))          # тип оцениваемой бинарной регрессии: в 
                                                                 # данном случае пробит

model_hetprobit <- hetglm(formula = default3 ~ log(income) +     # линейный индекс
                                              age + educ_3 |     # основного уравнения
                                              age + stable,      # линейный индекс
                                                                 # уравнения дисперсии
                          data = h,                                 
                          family = binomial(link = "probit"))
summary(model_hetprobit)
# В функции hetglm() link.scale указывает, 
# в каком виде представлена ошибка
# Имеются следующие варианты:
# 1. identity  ---  sigma_i        =  w_i * tau
# 2. log       ---  log(sigma_i)   =  w_i * tau  =>  sigma_i = exp(w_i * tau_i)
# 3. sqrt      ---  sqrt(sigma_i)  =  w_i * tau  =>  sigma_i = (w_i * tau_i) ^ 2

# Достанем полученные оценки
beta_est <- model_hetprobit$coefficients$mean                    # оценки коэффициентов при переменных
                                                                 # основного уравнения
tau_est <- model_hetprobit$coefficients$scale                    # оценки коэффициентов при переменных
                                                                 # в уравнении дисперсии

# Достанем оценки стандартных отклонений
# случайных ошибок
sigma_est <- predict(model_hetprobit, type = "scale")
head(sigma_est, 10)

# Осуществим тест на гомоскедастичность:
# H0: tau = 0
lrtest(model_hetprobit, model_probit) 

# Предскажем
prob_est <- predict(model_hetprobit, type = "response")          # вероятности
head(prob_est, 10)
y_li_est <- predict(model_hetprobit, type = "link")              # линейный индекс
head(y_li_est, 10)

# Рассчитаем предельный эффект
# для индивида
Boris <- data.frame(income = 55000,                              # укажем характеристики
                    age = 35,                                    # Бориса в датафрейме
                    educ_3 = 1,
                    stable = 1)
  # Предварительные расчеты
prob_Boris <- predict(model_hetprobit, newdata = Boris,          # оценка вероятности 
                      type = "response")                         # дефолта Бориса
li_Boris_adj <- predict(model_hetprobit, newdata = Boris,        # оценка отношения линейного
                        type = "link")                           # индекса Бориса к стнадртному
                                                                 # отклонению случайно ошибки
sigma_Boris <- predict(model_hetprobit, newdata = Boris,         # оценка стандартного
                       type = "scale")                           # отклонения случайной
                                                                 # ошибки Бориса
li_Boris <- li_Boris_adj * sigma_Boris                           # оценка линейного
                                                                 # индекса Бориса
  # Используем встроенную функцию
ME_Boris <- margins(model_hetprobit, 
                    data = Boris)
summary(ME_Boris)
  # Считаем предельный эффект аналитически   
ME_age_1 <- dnorm(li_Boris, sd = sigma_Boris) * 
                  (beta_est["age"] - 
                   li_Boris * tau_est["age"])
  # Считаем предельный эффект с помощью
  # численного дифференцирования
delta <- 1e-6                                                    # приращение                                                 
Boris_delta <- Boris
Boris_delta$age <- Boris$age + delta                             # приращение по возрасту
prob_Boris_delta <- predict(model_hetprobit, 
                            newdata = Boris_delta,
                            type = "response")
ME_age_2 <- (prob_Boris_delta - prob_Boris) / delta

# ЗАДАНИЯ (* - средне, ** - сложно, *** - очень сложно)
# 1.1. Используя встроенные данные Mroz87 из библиотеки
#      sampleSelection и пробит модель с гетероскедастичной
#      случайно ошибкой определите, как на вероятность
#      занятости (lfp) влияют возраст (age), образование (educ),
#      факт проживания в городе (city) и число несовершеннолетних
#      детей (kids5 и kids618). При этом предполагается, что
#      гетероскедастичность может быть обусловлена возрастом
#      и уровнем образования. Далее, для 28-летнего индивида 
#      без высшего образования и с доходом 20000 оцените:
#      1)    вероятность занятости
#      2)    предельный эффект возраста на вероятность занятости
#      3)    предельный эффект проживания в городе на вероятность занятости
#      4*)   предельный эффект возраста на вероятность занятости, если
#            возраст входит в линейный индекс квадратично
#      5)    повторите предыдущие пункты, используя различные подходы
#            к определению формы уравнения дисперсии: см. аргумент link.scale
#      6**)  стандартную ошибку оценки вероятности занятости

#---------------------------------------------------
# Дополнительные материалы
#---------------------------------------------------

# Проверим гипотезу о гомоскедастичности
# при помощи LM теста
HetprobitLnL <- function(x,                              # коэффициенты
                         y,                              # зависима переменна
                         X,                              # регрессоры основного уравнения
                         W,                              # регрессоры уравнения дисперсии
                         scale_fn = exp,                 # функция уравнения дисперсии
                         is_aggregate = TRUE)            # возвращаем функцию правдоподобия (TRUE)   
                                                         # или отдельные вклады (FALSE)          
{
  m_X <- ncol(X)
  m_W <- ncol(W)
  
  beta <- matrix(x[1:m_X], ncol = 1)                     # вектор beta коэффициентов и
  tau <- matrix(x[(m_X + 1):(m_X + m_W)], ncol = 1)      # вектор дополнительных параметров  
                                                         # переводим в матрицу с одним столбцом
  
  y_li_mean <- X %*% beta                                # оценка линейного индекса
  y_li_scale <- W %*% tau                                # латентной переменной
  y_li_scale_fn <- scale_fn(y_li_scale)
  
  n_obs <- nrow(X)                                       # количество наблюдений
  
  L_vec <- matrix(NA, nrow = n_obs,                      # вектор столбец вкладов наблюдений
                  ncol = 1)                              # в функцию правдоподобия
  
  is_y_0 <- (y == 0)                                     # вектор условий y = 0
  is_y_1 <- (y == 1)                                     # вектор условий y = 1
  
  L_vec[is_y_1] <- pnorm(y_li_mean[is_y_1], 
                         sd = y_li_scale_fn[is_y_1])     # вклад наблюдений для которых yi = 1
  L_vec[is_y_0] <- 1 - pnorm(y_li_mean[is_y_0],
                             sd = y_li_scale_fn[is_y_0]) # вклад наблюдений для которых yi = 0
  
  lnL_vec <- log(L_vec)                                  # логарифмы вкладов
  
  if(!is_aggregate)                                      # возвращаем вклады
  {                                                      # при необходимости
    return(lnL_vec)
  }
  
  lnL <- sum(lnL_vec)                                    # логарифм функции правдоподобия
  
  return(lnL)
}

# Достанем данные
df_hetprobit <- model.frame(model_hetprobit)             # все регрессоры
X_mat <- cbind(1, as.matrix(df_hetprobit[                # регрессоры основного
  names(df_hetprobit) %in% names(beta_est)]))            # уравнения
W_mat <- as.matrix(df_hetprobit[                         # регрессоры уравнения
  names(df_hetprobit) %in% names(tau_est)])              # дисперсии

# Достанем оценки ограниченной модели
x_est_R <- c(model_probit$coefficients, 
             rep(0, ncol(W_mat)))
n_R <- length(x_est_R)                                   # добавим имена
names(x_est_R)[(n_R - 1):n_R] <- paste(colnames(W_mat),  # для красоты
                                       "sigma")
print(x_est_R)

# Рассчитаем правдоподобие полной модели в точке,
# определяемой оценками, полученными по ограниченной
# модели
lnL_R <- HetprobitLnL(x_est_R, df_hetprobit[, 1],
                      X_mat, W_mat, exp)
lnL_R_grad <- grad(func = HetprobitLnL,                  # считаем градиент данной функции
                   x = x_est_R,                          # численным методом
                   y = df_hetprobit[, 1], 
                   X = X_mat, W = W_mat,
                   scale_fn = exp)                       # замените exp на function(x)
                                                         #                 {
                                                         #                   return(abs(x + 1)})
                                                         #                 }
                                                         # и убедитесь, что результат не изменится
lnL_R_grad <- matrix(lnL_R_grad, ncol = 1)               # градиент как матрица с одним столбцом
lnL_R_Jac <- jacobian(func = HetprobitLnL,               # оцениваем асимптотическую ковариационную
                      x = x_est_R,                       # матрицу при помощи Якобиана, расcчитанного
                      y = df_hetprobit[, 1],             # численным методом, поскольку численно
                      X = X_mat, W = W_mat,              # рассчитать Гессиан достаточно точным 
                      scale_fn = exp,                    # образом не получается
                      is_aggregate = FALSE,
                      method.args = list(r = 8))
as_cov_est <- solve(t(lnL_R_Jac) %*% lnL_R_Jac)          # cчитаем оценку асимптотической ковариационной
                                                         # матрицы с помощью Якобианов, поскольку 
                                                         # численным методом Гессиан считается
                                                         # очень плохо
# Реализуем тест
LM_value <- t(lnL_R_grad) %*%                            # считаем статистику теста
            as_cov_est %*%                               # множителей Лагранжа
            lnL_R_grad
p_value <- 1 - pchisq(LM_value, df = 2)                  # рассчитываем p-value теста

#---------------------------------------------------
# Часть 2. Тестирование гипотезы о нормальном
#          распределении случайных ошибок
#---------------------------------------------------

# -----
# Учимся:
# 1. Проверять гипотезу о нормальном распределении
#    случайно ошибки
# 2. Применять гибкие распределения случайных ошибок
# -----

# Оценим пробит модель
model_probit <- glm(formula = default2 ~ log(income) +
                                         age + I(age ^ 2) + 
                                         educ_3 + educ_2 +
                                         stable + 
                                         I(stable * log(income)) +
                                         sqrt(credit),
                    data = h,
                    family = binomial(link = "probit"))
summary(model_probit)

# Запишем функцию правдоподобия
# для модели со случайно ошибкой
# из распределения Пирсона
ProbitLnLExtended <- function(x,                         # вектор значений параметров
                              y,                         # зависимая переменная 
                              X,                         # матрица независимых переменных
                              is_aggregate = TRUE)       # при TRUE возвращаем логарифм
                                                         # функции правдоподобия, а при
                                                         # FALSE возвращаем вектор вкладов
{
  beta <- matrix(x[-c(1, 2)], ncol = 1)                  # вектор beta коэффициентов и
  t <- matrix(x[c(1, 2)], ncol = 1)                      # вектор дополнительных параметров  
                                                         # переводим в матрицу с одним столбцом
  y_li <- X %*% beta                                     # оценка линейного индекса
  y_est <- y_li + t[1] * y_li ^ 2 +                      # оценка математического ожидания 
                  t[2] * y_li ^ 3                        # латентной переменной
  
  n_obs <- nrow(X)                                       # количество наблюдений
  
  L_vec <- matrix(NA, nrow = n_obs,                      # вектор столбец вкладов наблюдений
                  ncol = 1)                              # в функцию правдоподобия
  
  is_y_0 <- (y == 0)                                     # вектор условий (y = 0)
  is_y_1 <- (y == 1)                                     # вектор условий (y = 1)
  
  L_vec[is_y_1] <- pnorm(y_est[is_y_1])                  # вклад наблюдений для которых yi = 1
  L_vec[is_y_0] <- 1 - pnorm(y_est[is_y_0])              # вклад наблюдений для которых yi = 0
  
  lnL_vec <- log(L_vec)                                  # логарифмы вкладов
  
  if(!is_aggregate)                                      # возвращаем вклады
  {                                                      # при необходимости
    return(lnL_vec)
  }
  
  lnL <- sum(lnL_vec)                                    # логарифм функции правдоподобия
  
  return(lnL)
}
# Воспользуемся созданной функцией
# Оценки модели при справедливом ограничении,
# накладываемом нулевой гипотезой
beta_est <- coef(model_probit)                           # достаем оценки из обычной пробит
beta_R <- c(0, 0, beta_est)                              # модели и приравниваем значения
names(beta_R)[c(1, 2)] <- c("t1", "t2")                  # дополнительных параметров к значениям,
# предполагаемым нулевой гипотезой
# Создадим матрицу регрессоров
X_mat <- as.matrix(model.frame(model_probit))            # достаем датафрейм с регрессорами и
X_mat[, 1] <- 1                                          # первращаем его в матрицу, а также
colnames(X_mat)[1] <- "Intercept"                        # заменяем зависимую переменную на константу
head(X_mat, 5)
# Применим функцию
lnL_R <- ProbitLnLExtended(beta_R, h$default2, X_mat)    # считаем логарифм функции правоподобия
# при ограничениях, совпадающую с логарифмом
# функции правдоподобия обычной пробит модели
lnL_R_grad <- grad(func = ProbitLnLExtended,             # считаем градиент данной функции
                   x = beta_R,                           # численным методом
                   y = h$default2, 
                   X = X_mat)
lnL_R_grad <- matrix(lnL_R_grad, ncol = 1)               # градиент как матрица с одним столбцом
lnL_R_Jac <- jacobian(func = ProbitLnLExtended,          # считаем Якобин данной функции
                      x = beta_R,                        # численным методом
                      y = h$default2, 
                      X = X_mat,
                      is_aggregate = FALSE)
as_cov_est <- solve(t(lnL_R_Jac) %*% lnL_R_Jac)          # cчитаем оценку асимптотической ковариационной
                                                         # матрицы с помощью Якобианов, поскольку 
                                                         # численным методом Гессиан считается
                                                         # в данном случае очень плохо
# Реализуем тест
LM_value_1 <- t(lnL_R_grad) %*%                          # считаем статистику теста
              as_cov_est %*%                             # множителей Лагранжа
              lnL_R_grad
p_value_1 <- 1 - pchisq(LM_value_1, df = 2)              # рассчитываем p-value теста
                                                         # множителей Лагранжа

# С использованием регрессии на единицы

# Достанем датафрейм, содержащий
# переменные модели
d <- model.frame(model_probit)                           # все переменные

# Рассчитаем предварительные величины
y_li_est <- predict(model_probit)                                              
F_est <- pnorm(y_li_est)                              
f_est <- dnorm(y_li_est)

# Вычислим обобщенные остатки
gr <- ((d[, 1] - F_est) /                                # обобщенный остаток
      (F_est * (1 - F_est))) * f_est

# Считаем производные по коэффициентам
d_beta <- apply(X_mat, 2, function(x)                    # производные по
{                                                        # регресионным коэффициентам
  x * gr
})
d_t1 <- (gr * y_li_est ^ 2)                              # производная по t1
d_t2 <- (gr * y_li_est ^ 3)                              # производная по t2

# Сравним аналитические и численные производные
grad_df <- data.frame("Numeric" = lnL_R_grad,
                      "Analytical" = colSums(cbind(d_t1, 
                                                   d_t2, 
                                                   d_beta)))
rownames(grad_df) <- c("t1", "t2", colnames(X_mat))
print(grad_df)

# Проводим LM тест
n <- nrow(d)                                             # число наблюдений
LM_df <- data.frame("my_ones" = rep(1, n),               # вектор из единиц 
                    "d_" = d_beta,
                    d_t1, d_t2)          
head(LM_df, 5)
ones_regression <- summary(lm(my_ones~. + 0,             # регрессия на вектор единиц без константы
                              data = LM_df))       
R2 <- ones_regression$r.squared                          # коэффициент детерминации регрессии
LM_value_2 <- R2 * n                                     # LM статистика
p_value_2 <- 1 - pchisq(q = LM_value_2, 
                        df = 2)

# Сравним полученные результаты и убедимся,
# что они полностью совпадают
c(LM_value_1, LM_value_2)                                # сравниваем статистики
c(p_value_1, p_value_2)                                  # сравниваем p-value

# Чтобы учесть возможность отклонения
# распределения от нормального можно
# воспользоваться моделью Галланта и Нички
model_hpa <- hpaBinary(formula = default2 ~ I(-log(income)) +
                                            age + I(age ^ 2) + 
                                            educ_3 + educ_2 +
                                            stable + 
                                            I(stable * log(income)) +
                                            sqrt(credit),
                       data = h,
                       K = 3)
# K - число параметров распределения Галланта и Нички
# При первом из регрессоров коэффициент фиксируется на
# единице, поэтому нужно использовать регрессорв,
# знак при котором предполагается положительным, либо
# сам знак самого регрессора меняется на противоположный
summary(model_hpa)
plot(model_hpa)                                          # оценка функции плотности
                                                         # случайных ошибок
plot(density(eps2))

# ЗАДАНИЯ (* - средне, ** - сложно, *** - очень сложно)
# 2.1. Осуществите тест на проверку соблюдения допущения
#      о нормальном распределении случайной ошибки
#      рассматривая в качестве регрессоров только
#      логарифм дохода и возраст используя:
#      1)    первый способ (функцию правдоподобия)
#      2)    второй способ (регрессия на вектор единиц)
# 2.2. Используя встроенные данные Mroz87 из библиотеки
#      sampleSelection определите, как на вероятность
#      занятости (lfp) влияют возраст (age), образование (educ),
#      факт проживания в городе (city) и число несовершеннолетних
#      детей (kids5 и kids618). Проверьте гипотезу о
#      нормальном распределении используя:
#      1)    первый способ (функцию правдоподобия)
#      2)    второй способ (регрессия на вектор единиц)